{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/habijanmarija/lab4/blob/main/lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8vtuqXEBP1j"
      },
      "source": [
        "# Lab 4 - Feature Detection (edge, corners, Hough transformation)\n",
        "\n",
        "These laboratory excersises are solved on Google Colab and are save on GitHub repo that is connected to GitHub Classroom.\n",
        "\n",
        "## Tools You need to use to Submit Assignments\n",
        "\n",
        "In this document, you will solve tasks. This is a Jupyter Notebook which has the **.ipynb** extension, is an interactive web environment for data analysis, visualization, solution presentations, education, and more.\n",
        "\n",
        "**Google Colab** is a tool that allows you to run and share Jupyter Notebook files on Google's servers, including the use of Google's CPU, GPU, and TPU resources. Colab is like Google Docs for Jupyter Notebooks. **Google Colab does not automatically save your assignment to GitHub.**\n",
        "\n",
        "**You use GitHub to save and submit your assignments.** When you accept the assignment through GitHub Classroom, a repository is automatically created on your GitHub account with a copy of the task. This is where you will save your solutions. Saving your solutions submits the tasks for that lab.\n",
        "\n",
        "## How to Solve the Tasks?\n",
        "1. Accept the task via the Google Classroom link that you will receive. Google Classroom will create a repository on your account.\n",
        "2. Go to the newly created repository on your account and click on the .ipynb file, then click Open in Colab.\n",
        "3. You will solve the tasks in Google Colab.\n",
        "\n",
        "## How to Save (Submit) Tasks?\n",
        "\n",
        "1. In Google Colab, click on the Open settings gear icon in the top-right corner.\n",
        "2. Click on the GitHub tab and check the box for Access private repositories and organizations.\n",
        "3. A new window will open for you to grant access to GitHub. For ferit-osirv, click Grant.\n",
        "4. Save and exit the settings.\n",
        "5. Click on File > Save a copy in GitHub.\n",
        "6. Select the lab repository that includes your name.\n",
        "\n",
        "> *Note:* You only need to complete steps 1-4 the first time.\n",
        "\n",
        "7. Click on **File > Save a copy in GitHub**.\n",
        "8. Select created repository **koji uključuje vaše ime**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feUPz7IDCbDx"
      },
      "source": [
        "## Copying Files from the GitHub Repository\n",
        "\n",
        "For completing the exercises, you will need images and other files that will be stored in the GitHub repository of the exercise. A command like this will be available in the notebook for each exercise. It will copy the files from GitHub to the Google Colab environment.\n",
        "\n",
        "**You need to run this command before starting each exercise.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpP_i0KgCefb",
        "outputId": "e08a0979-cb3c-41f4-be61-07957597ae31"
      },
      "outputs": [],
      "source": [
        "!rm -rf clone && git clone https://github.com/unisb-rv/labs/lab4 clone && cp -a clone/. ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIPg8Vf9Cr8D"
      },
      "source": [
        "**Google Colab will occasionally delete all files**. Therefore, you might need to rerun this command between sessions. If you encounter errors indicating that files do not exist, try running the command again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "EXS_YJC2WsWD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2 as cv\n",
        "import matplotlib.pyplot as plt\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Edge Detection\n",
        "\n",
        "When talking about images, the edge can be defined as a boundary between two regions with relatively distinct gray level properties.\n",
        "Edges are pixels where the brightness function changes abruptly.\n",
        "Edge detectors are a collection of very important local image preprocessing methods used to locate (sharpen) changes in the intensity function.\n",
        "Different edge detection methods include Canny, Sobel, Roberts, SUSAN, Prewitt, and Deriche.\n",
        "\n",
        "### Edge Detection with Canny Edge descriptor\n",
        "\n",
        "Canny Edge Detection is a popular edge detection algorithm. \n",
        "This is a multi-step algorithm, so its successful performance, depends on few steps: \n",
        "\n",
        "<ul>\n",
        "  <li><b>Preprocessing (noise reduction)</b></li>\n",
        "  Since all edge detection results are easily affected by image noise, it is essential to filter out the noise to prevent false detection caused by noise. \n",
        "  To smooth the image, we will use a Gaussian filter. \n",
        "  This step will slightly smooth the image to reduce the effects of obvious noise on the edge detector.\n",
        "  The equation for a Gaussian filter with a kernel of size (2k+1) x (2k+1) is given with: \n",
        "\n",
        "![gauss](https://wikimedia.org/api/rest_v1/media/math/render/svg/4a36d7f727beeaff58352d671bb41a3aca9f44d6)\n",
        "  \n",
        "  As seen from the above formula, important parameters for the Gaussian filter are:\n",
        "  <ul>\n",
        "    <li>size of the kernel (mostly 5x5 kernel is used)</li>\n",
        "    <li>and standard deviation sigma </li>\n",
        "  </ul>\n",
        "  \n",
        "   <p>\n",
        "  <li><b>Finding intensity gradient of the image</b></li>\n",
        "  An edge in an image may point in a variety of directions, so the Canny algorithm uses filters to detect horizontal, vertical and diagonal edges in the blurred image. \n",
        "  The edge detection operator (such as Roberts, Prewitt, or Sobel) returns a value for the first derivative in the horizontal direction (Gx) and the vertical direction (Gy). \n",
        "  In other words, <i>the magnitude</i> of the gradient at a point in the image determines if it possibly lies on the edge or not. A high gradient magnitude means the colors are changing rapidly which implies the existence of an edge while a low gradient implies that edge is not present. \n",
        "  The <i>direction</i> of the gradient shows how the edge is oriented.\n",
        "  To calculate these, following formulas are used: \n",
        "\n",
        "![canny-edge-1](http://latex.codecogs.com/gif.latex?Edge%20Gradient%20%28G%29%20%3D%20%5Csqrt%7BG_x%5E2&amp;plus;G_y%5E2%7D)\n",
        "![canny-edge-2](http://latex.codecogs.com/gif.latex?Angle%20%28%5Ctheta%29%20%3D%20%5Ctan%5E%7B-1%7D%5Cfrac%7BG_y%7D%7BG_x%7D)\n",
        "\n",
        "  Once we have the gradient magnitudes and orientations, we can get started with the actual edge detection.\n",
        " \n",
        " </p>\n",
        "\n",
        " <p>\n",
        "  <li><b>Applying non-maximum suppression to get rid of spurious response to the edge detection</b></li>\n",
        "\n",
        "  After gradient magnitude and direction are obtained, a full scan of the image is done to remove any unwanted pixels which may not constitute the edge.\n",
        "  Therefore, edge thining technique known as non-maximum suppression is applied to find all those unwanted pixels. \n",
        "  For this, at every pixel, the pixel is checked if it is a local maximum in its neighborhood in the direction of the gradient. Check the image below: \n",
        "\n",
        "![non-maximum](https://docs.opencv.org/3.1.0/nms.jpg)\n",
        "  <br>\n",
        "  Point A is on the edge ( in the vertical direction). Gradient direction is normal to the edge. Point B and C are in gradient directions. \n",
        "  So point A is checked with point B and C to see if it forms a local maximum. If so, it is considered for the next stage, otherwise, it is suppressed ( put to zero).\n",
        "  In short, the result you get is a binary image with \"thin edges\".\n",
        "</p>\n",
        "\n",
        "  <li><b>Track edge by hysteresis </li></b>\n",
        "  Finalize the detection of edges by suppressing all the other edges that are weak and not connected to strong edges.\n",
        "  This stage decides which are all edges are really edges and which are not. For this, we need two threshold values, minVal, and maxVal. \n",
        "  Any edges with intensity gradient more than maxVal are sure to be edges and those below minVal are sure to be non-edges, so discarded. \n",
        "  Those who lie between these two thresholds are classified edges or non-edges based on their connectivity. \n",
        "  If they are connected to \"sure-edge\" pixels, they are considered to be part of edges. Otherwise, they are also discarded. See the image below:\n",
        "\n",
        "![non-maximum](https://docs.opencv.org/3.1.0/hysteresis.jpg)\n",
        "\n",
        " <p>\n",
        "    The edge A is above the maxVal, so considered as \"sure-edge\". Although edge C is below maxVal, it is connected to edge A, so that also considered as valid edge and we get that full curve. \n",
        "    But edge B, although it is above minVal and is in the same region as that of edge C, it is not connected to any \"sure-edge\", so that is discarded. \n",
        "    So it is very important that we have to select minVal and maxVal accordingly to get the correct result. \n",
        "    This stage also removes small pixels noises on the assumption that edges are long lines. So what we finally get is strong edges in the image.\n",
        " </p>\n",
        "  \n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "OpenCV implementation of Canny edge detector:\n",
        "\n",
        "```\n",
        "Function : cv2.Canny(blurred image,lower threshold,upper threshold)\n",
        "Parameters are as follows :\n",
        "1. blurred image : input image blurred with Gaussian 5 by 5 kernel\n",
        "2. lower threshold : first threshold for the hysteresis procedure\n",
        "3. upper threshold : second threshold for the hysteresis procedure\n",
        "```\n",
        "More information can be found at:  <a href=\"https://docs.opencv.org/3.0-beta/modules/imgproc/doc/feature_detection.html?highlight=cv2.canny#cv2.Canny\">OpenCV cv2.Canny documentation</a> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1\n",
        "\n",
        "Try the Canny edge detector on an arbitrary image within the `slike` directory. **Load the image as a grayscale image.** Also, before applying Canny edge detection, blur the image using **Gaussian blur**. (Refer to previous exercises.) Display the original image and the edge-detected image. Visually determine the values of the lower and upper thresholds to obtain clear edges without too many extra ones. Good starting values are between 100 and 200."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you could observe, the common problem here is in determining these lower and upper thresholds. So, what is the optimal value for the thresholds?\n",
        "This question is especially important when you are processing multiple images with different contents captured under varying lighting conditions.  \n",
        "A little trick that relies on basic statistics can help you automatically determ these values, removing the manual tuning of the thresholds for Canny edge detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hough Transformation for Lines\n",
        "\n",
        "When images are to be used in different areas of image analysis such as object recognition, it is important to reduce the amount of data in the image while preserving the important,\n",
        "characteristic, structural information. Edge detection makes it possible to reduce the amount of data in an image considerably. However the output from an edge detector is still a image\n",
        "described by it’s pixels. If lines, ellipses and so forth could be defined by their characteristic equations, the amount of data would be reduced even more. \n",
        "The Hough transform was originally developed to recognize lines and has later been generalized to cover arbitrary shapes. \n",
        "\n",
        "<ul>\n",
        "<li> <b>Representation of Lines in the Hough Space</b> </li>\n",
        "\n",
        "Lines can be represented uniquely by with parameters a and b and following equation: \n",
        "\n",
        "![line-1](https://latex.codecogs.com/gif.latex?y%3Da%5Ccdot%20x%20&plus;b)\n",
        "\n",
        "Above equation is not able to represent vertical lines. Therefore, the Hough transform uses the following equation: <br>\n",
        "\n",
        "![line-2](https://latex.codecogs.com/gif.latex?r%20%3D%20x%20%5Ccdot%20%5Ccos%20%5CTheta%20&plus;%20y%5Ccdot%20%5Csin%20%5CTheta)\n",
        "\n",
        "To obtain similar equation to the first one, this can be rewritten as: \n",
        "\n",
        "![line-3](https://latex.codecogs.com/gif.latex?y%20%3D%20-%20%5Cfrac%7B%5Ccos%20%5CTheta%20%7D%7B%5Csin%20%5CTheta%20%7D%20%5Ccdot%20x%20&plus;%20%5Cfrac%7Br%7D%7B%5CTheta%20%7D)\n",
        "\n",
        "The parameters ![line-1](https://latex.codecogs.com/gif.latex?%5Ctheta) and r is the angle of the line and the distance from the line to the origin, respectively. \n",
        "All lines can be represented in this form when  <img src=\"https://i.ibb.co/4Sq4DkD/formula1.png\" alt=\"hough1\" border=\"0\"></a>.\n",
        "To sum up, the Hough space for lines has these two dimensions: ![line-1](https://latex.codecogs.com/gif.latex?%5Ctheta) and r and a line is represented\n",
        "by a single point, corresponding to a unique set of parameters ![line-1](https://latex.codecogs.com/gif.latex?%28%5Ctheta%20_%7B0%7D%2Cr_%7B0%7D%29) . \n",
        "The line-to-point mapping is illustrated in the following image: \n",
        "<p align=\"center\">\n",
        "<img src=\"https://i.ibb.co/gJ9C8Jy/hough1.png\" alt=\"hough1\" border=\"0\"></a><br /><br />\n",
        "</p>\n",
        "\n",
        "<li> <b>Mapping of edge points to the Hough space</b> </li>\n",
        "\n",
        "An important concept for the Hough transform is the mapping of single points. The idea is, that a point is mapped to all lines, that can pass through that point.\n",
        "This yields a sine-like line in the Hough space.  This principle is illustrated for a point ![formula](https://latex.codecogs.com/gif.latex?p_%7B0%7D%3D%20%2840%2C30%29)as shown in following figures: \n",
        "<p align=\"center\">\n",
        "<img src=\"https://i.ibb.co/GVRDyJs/houghspace.png\" alt=\"houghspace\" border=\"0\">\n",
        "</p>\n",
        "On the left image transformation of a single point to a line in the Hough space is shown while on the right image \n",
        "the Hough space line representation through all possible lines through the point is shown. \n",
        "\n",
        "#### The Hough Space Accumulator\n",
        "\n",
        "To determine the areas where most Hough space lines intersect, an accumulator covering the Hough space is used. When an edge point is transformed, bins in the accumulator is incremented\n",
        "for all lines that could pass through that point. The resolution of the accumulator determines the precision with which lines can be detected. \n",
        "In general, the number of dimensions of the accumulator corresponds to the number of unknown parameters in the Hough transform problem. Thus, for ellipses a 5-dimensional space is required\n",
        "(the coordinates of its center, the length of its major and minor axis, and its angle). For lines 2 dimensions suffice (r and θ). This is why it is possible to visualize the content of the ac\n",
        "cumulator.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To sum up, the algorithm for detecting straight lines can be divided to the following steps: \n",
        "<ul>\n",
        "<li> Edge detection, e.g. using the Canny edge detector </li>\n",
        "<li> Mapping of edge points to the Hough space and storage in an accumulator </li>\n",
        "<li> Interpretation of the accumulator to yield lines of infinite length. The interpretation isdone by thresholding and possibly other constraints. </li>\n",
        "<li> Conversion of infinite lines to finite lines. </li>\n",
        "</ul>\n",
        "\n",
        "More information about Hough Transformation for lines can be found at:  <a href=\"href> https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_imgproc/py_houghlines/py_houghlines.html\">OpenCV Hough Transform documentation</a> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "OpenCV implementation of Hough Transform for lines can be found in: \n",
        "\n",
        "```\n",
        "Function : cv2.HoughLines(image with edges,rho,theta,threshold, array,srn,stn)\n",
        "Parameters are as follows :\n",
        "1. image with edges : input image with found edges\n",
        "2. rho : distance resolution of the accumulator in pixels\n",
        "3. theta : angle resolution of the accumulator in radians\n",
        "4. threshold: accumulator threshold parameter. only those lines are returned that get enough votes \n",
        "5. array: return an empty array with shape and type of input for storing result\n",
        "6. srn: for the multi-scale Hough transform, if 0 standard Hough transform is used\n",
        "7. stn: for the multi-scale Hough transform, if 0 standard Hough transform is used\n",
        "```\n",
        "More information can be found at:  <a href=\"https://docs.opencv.org/3.0-beta/modules/imgproc/doc/feature_detection.html?highlight=cv2.hough#cv2.HoughLines\">OpenCV cv2.HoughLines documentation</a> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2\n",
        "\n",
        "On the crossword.jpg image, perform the Hough transformation. Since the input to the HoughLines function is an edge-detected image, you first need to perform Canny edge detection as in the first task, including the blur step. Then, perform the Hough transformation on the edge image. After completing the transformation, call the draw_lines function to draw the detected lines on the original image. Display the resulting image. Example call:\n",
        "\n",
        "`lines = cv2.HoughLines(edges, 1, math.pi/90, 200, np.array([]), 0, 0)`\n",
        "\n",
        "These are good starting values for the parameters. Adjust the threshold parameter, which is set to 200 in the example call, to get the lines of the crossword."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def draw_lines(img, lines):\n",
        "  a,b,c = lines.shape\n",
        "  for i in range(a):\n",
        "      rho = lines[i][0][0]\n",
        "      theta = lines[i][0][1]\n",
        "      a = math.cos(theta)\n",
        "      b = math.sin(theta)\n",
        "      x0, y0 = a*rho, b*rho\n",
        "      pt1 = ( int(x0+1000*(-b)), int(y0+1000*(a)) )\n",
        "      pt2 = ( int(x0-1000*(-b)), int(y0-1000*(a)) )\n",
        "      cv.line(img, pt1, pt2, (255, 0, 0), 2, cv.LINE_AA)\n",
        "  return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hough transform slike crossword.jpg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 3\n",
        "\n",
        "Try the same code as above, but set the value of the `theta` parameter to `math.pi/180`. Notice the difference in which lines are detected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Corner Detection \n",
        "\n",
        "Corners are locations in images where a slight shift in the location will lead to a large change in intensity in both horizontal and vertical axes.\n",
        "The Harris Corner detection algorithm consist of following steps:\n",
        "\n",
        "<ul>\n",
        "<li><b>Determination of windows (small image patches) with large intensity variation</b></li>\n",
        "\n",
        "Let a window (the center) be located at position  &nbsp; ![cornerr](https://latex.codecogs.com/gif.latex?%28x%2Cy%29).  &nbsp; \n",
        "Let the intensity of the pixel at this location be  &nbsp; ![cornerrr](https://latex.codecogs.com/gif.latex?I%28x%2Cy%29).  &nbsp;\n",
        "If this window slightly shifts to a new location with displacement  &nbsp; ![corner7](https://latex.codecogs.com/gif.latex?%28u%2Cv%29),  &nbsp; the intensity of the pixel at this location will be \n",
        " &nbsp; ![corner8](https://latex.codecogs.com/gif.latex?I%28x&plus;u%2Cy&plus;v%29).  &nbsp; \n",
        "\n",
        "Therefore,  &nbsp; ![corner9](https://latex.codecogs.com/gif.latex?%5BI%28x&plus;u%2Cy&plus;v%29-I%28x%2Cy%29%5D)  &nbsp; will be the difference in intensities of the window shift. \n",
        "For a corner, this difference will be very high. \n",
        "We maximize this term by differentiating it with respect to the X and Y axes. \n",
        "Let  &nbsp; ![corner10](https://latex.codecogs.com/gif.latex?w%28x%2Cy%29)  &nbsp;be the weights of pixels over a window (Rectangular or a Gaussian).\n",
        "Then,  &nbsp; ![corner11](https://latex.codecogs.com/gif.latex?E%28u%2Cv%29)  &nbsp; is defined as :\n",
        "<p align=\"center\">\n",
        "<img src=\"https://cdn-images-1.medium.com/max/800/0*v4pgxvEFE8JvroJv.png\" alt=\"hough1\" border=\"0\"></a><br /><br />\n",
        "</p>\n",
        "\n",
        "\n",
        "Since,computing &nbsp; ![corner11](https://latex.codecogs.com/gif.latex?E%28u%2Cv%29)  &nbsp; will be computationally challenged, optimisation with Taylor series expansion (only the 1rst order)\n",
        "is applyed. Some math leads us to: <br/>\n",
        "\n",
        "![corner12](https://latex.codecogs.com/gif.latex?E%28u%2Cv%29%5Capprox%20%28u%2Cv%29M%5Cbinom%7Bx%7D%7By%7D). &nbsp; \n",
        "\n",
        "And finally structure tensor is defined with :\n",
        "<p align=\"center\">\n",
        "&nbsp;<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/a617dda21e306dbfbdb7a186b1c203e3f3443867\" alt=\"hough1\" border=\"0\"></a><br /><br />\n",
        "</p>\n",
        "\n",
        "<li><b>Computation of score R for each found window</b></li>\n",
        "After fiding windows with large variations, selection of suitable corners is performed. \n",
        "It was estimated that the eigenvalues of the matrix can be used to do this. Calculation of a score associated with each such window is given with:  <br/>\n",
        "\n",
        "![corner2](https://latex.codecogs.com/gif.latex?R%20%3D%20det%28M%29%20-%20k%5Ccdot%20%28trace%28M%29%29%5E%7B2%7D)   <br/>\n",
        "\n",
        "where &nbsp; ![corner2](https://latex.codecogs.com/gif.latex?det%28M%29%20%3D%20%5Clambda%20_%7B1%7D%20%5Ccdot%20%5Clambda_%7B2%7D) &nbsp; and &nbsp; \n",
        "&nbsp; ![corner3](https://latex.codecogs.com/gif.latex?trace%28M%29%20%3D%20%5Clambda%20_%7B1%7D%20&plus;%20%5Clambda_%7B2%7D).  &nbsp;\n",
        "Here,  &nbsp; ![corner4](https://latex.codecogs.com/gif.latex?%5Clambda%20_%7B1%7D) &nbsp; and  &nbsp; ![corner5](https://latex.codecogs.com/gif.latex?%5Clambda%20_%7B2%7D)  &nbsp; \n",
        "are eigenvalues of M, and k is an empirical constant. \n",
        "\n",
        "\n",
        "<li><b>Applying a threshold to the score R and important corners selection</b></li>\n",
        "\n",
        "Depending on the value of R, the window is classified as consisting of flat, edge, or a corner. \n",
        "A large value of R indicates a corner, a negative value indicates an edge. \n",
        "Also, in order to pick up the optimal values to indicate corners, we find the local maxima as corners within the window which is a 3 by 3 filter.\n",
        "\n",
        "</ul>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "OpenCV implementation of Harris Corner detector can be found in: \n",
        "\n",
        "```\n",
        "Function : cv2.cornerHarris(image,blocksize,ksize,k)\n",
        "Parameters are as follows :\n",
        "1. image : the source image in which we wish to find the corners (grayscale)\n",
        "2. blocksize : size of the neighborhood in which we compare the gradient \n",
        "3. ksize : aperture parameter for the Sobel() Operator (used for finding Ix and Iy)\n",
        "4. k : Harris detector empirical constant parameter (used in the calculation of R)\n",
        "```\n",
        "More information can be found at:  <a href=\"https://docs.opencv.org/3.0-beta/modules/imgproc/doc/feature_detection.html?highlight=cv2.cornerharris#cv2.cornerHarris\">OpenCV cv2.cornerHarris documentation</a> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 4\n",
        "\n",
        "On the image `images/crossword.jpg`, perform corner detection so that the corners of the crossword are visible. Load the image as **grayscale** and call the cornerHarris function on it. Example call with good initial parameter values:\n",
        "\n",
        "`corners = cv.cornerHarris(img, 15, 3, 0.05)`\n",
        "\n",
        "Then set the original image to 255 wherever the corners image is greater than a certain value. Note that the corners image will contain very small values, so a good starting threshold is around 0.0001. Recall the early labs for image thresholding, specifically setting image values where another image is greater than a certain value. Display the resulting image. Try modifying the parameters blockSize, k, or the threshold to better highlight the corners of the crossword."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By combining the Canny edge detector and Harris corner detector, we can segment different parts of images. For example, we can count blood cells in microscopic images, find solutions to Sudoku puzzles, crosswords, or in chess, detect cracks and anomalies in various materials or roads, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced Feature Detection \n",
        "\n",
        "Now we will take a look at more two topics: advanced feature detection extraction algorithms and object detection (using traditional image processing methods). \n",
        "\n",
        "Feature extraction is a process of dimensionality reduction by which an initial set of raw data is reduced to more manageable groups for processing. A characteristic of these large data sets is a large number of variables that require a lot of computing resources to process. Feature extraction is the name for methods that select and /or combine variables into features, effectively reducing the amount of data that must be processed, while still accurately and completely describing the original data set.\n",
        "\n",
        "Object detection is a technique that works to identify and locate objects within an image or video. In this way it provides better understanding and analisys of scenes in images and videos. Specifically, object detection draws bounding boxes around these detected objects, which allow us to locate where said objects are in (or how they move through) a given scene. With this kind of identification and localization, object detection can be used to count objects in a scene and determine and track their precise locations, all while accurately labeling them. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Detection and Extraction\n",
        "\n",
        "What is feature?\n",
        "A local image feature is a tiny patch in the image that's invariant to image scaling, rotation and change in illumination.\n",
        "It's like the tip of a tower, or the corner of a window in the image above. Unlike a random point on the background (sky) in the image above,\n",
        "the tip of the tower can be precise detected in most images of the same scene. It is geometricly (translation, rotation, ...) and photometricly (brightness, exposure, ...) invariant.\n",
        "A good local feature is like the piece you start with when solving a jigsaw puzzle, except on a much smaller scale.\n",
        "It's the eye of the cat or the corner of the table, not a piece on a blank wall.\n",
        "The extracted local features must be:\n",
        "\n",
        "* Repeatable and precise so they can be extracted from different images showing the same object.\n",
        "* Distinctive to the image, so images with different structure will not have them.\n",
        "\n",
        "Due to these requirements, most local feature detectors extract corners and blobs. There is a wealth of algorithms satisfying the above requirements for feature detection (finding interest points on an image) and description\n",
        "(generating a vector representation for them). They include already learned Harris Corner Detection (in lab 4), and some more advanced algorithms, such as: \n",
        "\n",
        "* Scale Invariant Feature Transform (SIFT)\n",
        "* Speeded-Up Robust Features (SURF)\n",
        "* Features from Accelerated Segment Test (FAST)\n",
        "* Binary Robust Independent Elementary Features (BRIEF)\n",
        "* Oriented FAST and rotated BRIEF (ORB)\n",
        "\n",
        "The SIFT and SURF algorithms are patented by their respective creators, and while they are free to use in academic and research settings, you should technically be obtaining a license/permission from the creators if you are using them in a commercial (i.e. for-profit) application.\n",
        "\n",
        "Since is a known fact that ORB performs as well as SIFT on the task of feature detection (while outperforms SURF), in this lab, our focus will be on Oriented FAST and rotated BRIED (ORB)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Oriented FAST and rotated BRIEF (ORB) \n",
        "\n",
        "Oriented FAST and rotated BRIEF (ORB) is a fast robust local feature detector. It is basically a fusion of FAST keypoint detector and BRIEF descriptor\n",
        "with many modifications to enhance the performance.\n",
        "\n",
        "ORB is a fusion of FAST keypoint detector and BRIEF descriptor with some added features to improve the performance. FAST is Features from Accelerated Segment Test used to detect features from the provided image. It also uses a pyramid to produce multiscale-features. Now it doesn’t compute the orientation and descriptors for the features, so this is where BRIEF comes in the role.\n",
        "\n",
        "ORB uses BRIEF descriptors but as the BRIEF performs poorly with rotation. So what ORB does is to rotate the BRIEF according to the orientation of keypoints. Using the orientation of the patch, its rotation matrix is found and rotates the BRIEF to get the rotated version. ORB is an efficient alternative to SIFT or SURF algorithms used for feature extraction, in computation cost, matching performance, and mainly the patents. SIFT and SURF are patented and you are supposed to pay them for its use. But ORB is not patented.\n",
        "\n",
        "\n",
        "We’ll start by showing the following figure that shows an example of using ORB to match between real world images with viewpoint change. Green lines are valid matches, red circles indicate unmatched points.\n",
        "\n",
        "ORB  uses an orientation compensation mechanism, making it rotation invariant while learning the optimal sampling pairs.\n",
        "\n",
        "### Orientation Compensation\n",
        "\n",
        "ORB uses a simple measure of corner orientation – the intensity centroid [5]. First, the moments of a patch are defined as:\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://gilscvblog.files.wordpress.com/2013/10/figure2.jpg\">\n",
        "</p>\n",
        "\n",
        "With these moments we can find the centroid, the “center of mass” of the patch as:\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://gilscvblog.files.wordpress.com/2013/10/figure3.jpg?w=300&h=116\">\n",
        "</p>\n",
        "\n",
        "We can construct a vector from the corner’s center O, to the centroid -OC. The orientation of the patch is then given by:\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://gilscvblog.files.wordpress.com/2013/10/figure4.jpg?w=300&h=53\">\n",
        "</p>\n",
        "\n",
        "Here is an illustration to help explain the method:\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://gilscvblog.files.wordpress.com/2013/10/angle.jpg\">\n",
        "</p>\n",
        "\n",
        "\n",
        "Once we’ve calculated the orientation of the patch, we can rotate it to a canonical rotation and then compute the descriptor, thus obtaining some rotation invariance.\n",
        "\n",
        "We will now see ORB in action through examples and assigments! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Try out the following code in order to find corners on the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [10, 8] # enlarge inline plots \n",
        "\n",
        "# load input image\n",
        "img = cv2.imread('images/building_1.jpg')\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  \n",
        "plt.plot,plt.imshow(img)\n",
        "plt.title('Input Image'), plt.xticks([]), plt.yticks([])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create ORB \n",
        "orb = cv2.ORB_create()\n",
        "# we can create ORB with specific number of key points we desire, for example, try to set them to 1000\n",
        "#orb = cv2.ORB_create(1000, 1.2)\n",
        "key_points, description = orb.detectAndCompute(img, None)\n",
        "print(\"Number of keypoints Detected: \", len(key_points))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# draw rich key points on input image \n",
        "img_keypoints =cv2.drawKeypoints(img,key_points,img,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# draw result image:\n",
        "plt.plot,plt.imshow(img_keypoints)\n",
        "plt.title('ORB Interest Point'), plt.xticks([]), plt.yticks([])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The found interest points/features are circled in the image above.\n",
        "As we can see, some of these points are unique to this scene/building like the points near the top of the two towers. However, others like the ones at the top of the tree may not be distinctive. \n",
        "\n",
        "In the next assigment, you will try to extract the same features from a different image of the same cathedral taken from a different angle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Assigment 1 - Feature Detection with ORB detector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [20, 20] # enlarge inline plots "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load and plot input image named 'building_1.jpg'\n",
        "#TODO:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load input image named 'building_2.jpg'\n",
        "#TODO:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create ORB detector\n",
        "#TODO:\n",
        "orb = \n",
        "# find keypoints and their descriptors \n",
        "#TODO:\n",
        "key_points, description = \n",
        "# draw rich key points on input image \n",
        "#TODO:\n",
        "img_keypoints ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# feature extraction function\n",
        "def image_detect_and_compute(detector, img_name):\n",
        "    # detect and compute intetrest points and their descriptors\n",
        "    img = cv2.imread(img_name)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    kp, des = detector.detectAndCompute(img, None)\n",
        "    return img, kp, des"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def draw_image_matches(detector, img1_name, img2_name, nmatches=20):\n",
        "    # draw ORB feature matches of the given two images, using previously defined function\n",
        "    # TODO:\n",
        "    img1, kp1, des1 = \n",
        "    img2, kp2, des2 = \n",
        "\n",
        "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
        "    matches = bf.match(des1, des2)\n",
        "    matches = sorted(matches, key = lambda x: x.distance)\n",
        "    img_matches = cv2.drawMatches(img1, kp1, img2, kp2, matches[:nmatches],img2, flags=2)\n",
        "    # draw result images\n",
        "    plt.subplot(121),plt.imshow(img_keypoints)\n",
        "    plt.title('ORB Interest Point'), plt.xticks([]), plt.yticks([])\n",
        "    plt.subplot(122),plt.imshow(img_matches)\n",
        "    plt.title('Detector'), plt.xticks([]), plt.yticks([])\n",
        "    plt.show()\n",
        "# draw image matches\n",
        "orb = cv2.ORB_create()\n",
        "draw_image_matches(orb, 'images/building_1.jpg','images/building_2.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Using Convolutions for Feature Detection\n",
        "\n",
        "## Image Convolution\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "\n",
        "![convolution](https://i.postimg.cc/x1nPhpHy/conv.png)\n",
        "\n",
        "Generally, convolution is a mathematical operation between two functions. In the context of this assignment, however, we will focus on discrete 2D convolution between two square images, as that is most relevant for image processing. Convolution is denoted as $I(A) \\star k(B)$ where $I(A)$ is an image $I(A) \\in \\mathbb{R}^{W \\times H}$ and $k(B)$ is a matrix $k(B) \\in \\mathbb{R}^{a \\times b}$ indexed by locations $B \\in \\mathbb{N}^2$ called the **convolutional kernel**. At pixel $(x, y)$, the convolution operation is defined as:\n",
        "\n",
        "\\begin{equation}\n",
        "(I \\star k)(x, y) = \\sum_{i=-a}^{a} \\sum_{j=-b}^{b} I[x + i, y + j] k[i, j]\n",
        "\\end{equation}\n",
        " \n",
        "Explained differently, the resulting image is produced by sliding the kernel over the input image pixel by pixel. At each pixel location, values where the kernel and the image overlap are multiplied, and all of the products are summed together to form the corresponding pixel's value in the output image.\n",
        "\n",
        "While mathematically a simple operation, convolution is exceedingly powerful and can produce almost endless transformations of an image. It's most commonly used for filtering --- a convolution can elegantly find patterns in the image and increase their intensity. One such example is the convolution with a kernel called the Prewitt operator:\n",
        "\n",
        "\\begin{equation}\n",
        "I_y(A) = I(A) \\star \\begin{bmatrix}\n",
        "-1 & 0 & 1\\\\\n",
        "-1 & 0 & 1\\\\\n",
        "-1 & 0 & 1\\\\\n",
        "\\end{bmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "When convolved with this kernel, the resulting image has high-intensity pixels in regions where vertical edges are present, and low intensity everywhere else. This can be seen in the following image:\n",
        "\n",
        "![prewitt](https://i.postimg.cc/X7cffKhZ/prewitt-example.png)\n",
        "\n",
        "Vertical edges necessarily have to have a large jump in values going from left to right or right to left. Otherwise, there would be no perceptible edge. This kernel takes advantage of that fact to accentuate parts of the image where there is such a jump. It does this by replacing each pixel with the difference between the pixels on its left and its right.\n",
        "\n",
        "This process happens as follows. For each pixel of the input image, the kernel is placed such that it is centered on that pixel. This means that the values of the pixel as well as its neighbors above and below are all multiplied by zero. The neighbors on the left are multiplied by -1, and the ones on the right are multiplied by 1. Summed together, the result represents the sum of the values on the right of the pixel, minus the sum of the values on the left.\n",
        "\n",
        "To illustrate this, let us consider $1 \\times 3$ region of the image where no vertical edges are present:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{bmatrix}\n",
        "128 & 130 & 136\\\\\n",
        "\\end{bmatrix}\n",
        "\\star\n",
        "\\begin{bmatrix}\n",
        "-1 & 0 & 1\\\\\n",
        "-1 & 0 & 1\\\\\n",
        "-1 & 0 & 1\\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\sum_{i,j}\n",
        "\\begin{bmatrix}\n",
        "-1 \\times 0 + 0 \\times 128 + 1 \\times 130\\\\ \n",
        "-1 \\times 128 + 0 \\times 130 + 1 \\times 136\\\\\n",
        "-1 \\times 130 + 0 \\times 136 + 1 \\times 0\\\\\n",
        "\\end{bmatrix}\n",
        "= 8\n",
        "\\end{equation}\n",
        "\n",
        "This section of the image does not contain a vertical edge, so the convolution result is a relatively low value. In a standard image with values in $[0, 255)$, 8 would appear almost completely black.\n",
        "\n",
        "However, consider some section of the image where a vertical edge is indeed present:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{bmatrix}\n",
        "63 & 66 & 132\\\\\n",
        "\\end{bmatrix}\n",
        "\\star\n",
        "\\begin{bmatrix}\n",
        "-1 & 0 & 1\\\\\n",
        "-1 & 0 & 1\\\\\n",
        "-1 & 0 & 1\\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\sum_{i,j}\n",
        "\\begin{bmatrix}\n",
        "-1 \\times 0 + 0 \\times 64 + 1 \\times 66\\\\ \n",
        "-1 \\times 64 + 0 \\times 66 + 1 \\times 132\\\\\n",
        "-1 \\times 66 + 0 \\times 132 + 1 \\times 0\\\\\n",
        "\\end{bmatrix}\n",
        "= 68\n",
        "\\end{equation}\n",
        "\n",
        "The value is now much larger due to the difference between the left and right sides of the image. This example demonstrates how a relatively simple kernel can capture complex features of an image.\n",
        "\n",
        "Beyond edge detection, there are many commonly used convolution kernels to perform tasks such as blurring, sharpening, or denoising images. A convolutional neural network can leverage the power of the convolution by stringing together sequences of intricate kernels to match complex patterns in the image.\n",
        "\n",
        "### Resources\n",
        "\n",
        "- https://en.wikipedia.org/wiki/Kernel_(image_processing)\n",
        "- https://vincmazet.github.io/bip/filtering/convolution.html\n",
        "- https://arxiv.org/pdf/1603.07285.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will first implement a convolution operation from scratch. Convert the following equation into code:\n",
        "\n",
        "\\begin{equation}\n",
        "(I \\star k)(x, y) = \\sum_{i=1}^{a} \\sum_{j=1}^{b} I[x + i, y + j] k[i, j].\n",
        "\\end{equation}\n",
        "\n",
        "This equation defines the convolution value for a single pixel $(x, y)$, where $I$ is the image and $k$ is a kernel of size $a \\times b$. You will need to perform this operation for each pixel. Complete `convolve_py` by returning a convolved image.\n",
        "\n",
        "**Finish the code blocks below at the TODO comments. You do not need to change the rest of the code, or code blocks that do not have TODO comments.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convolve_py(img, kernel):\n",
        "  '''\n",
        "  Convolves an image with a kernel.\n",
        "  Uses pure Python (no Numpy).\n",
        "\n",
        "  Args:\n",
        "    img: 2D numpy array of the image (grayscale, e.g. (128,128))\n",
        "    kernel: 2D numpy array of the kernel\n",
        "  '''\n",
        "  # Get the dimensions of the image and kernel\n",
        "  img_height, img_width = img.shape\n",
        "  kernel_height, kernel_width = kernel.shape\n",
        "\n",
        "  conv_result = np.zeros(img.shape)\n",
        "  img = np.pad(img, (1, 1), 'constant', constant_values=(0, 0))\n",
        "\n",
        "  for x in range(img_width):\n",
        "    for y in range(img_height):\n",
        "      # TODO: compute pixel (x, y) using equation above\n",
        "      # Note: If the kernel goes outside the boundary of the image, assume that the values outside the image are zero.\n",
        "      # Note: In mathematical notation, matrix indices start at 1. In Python, matrix indices start at 0.\n",
        "      # Note: The `img` pixel (x, y) is `img[y, x]`.\n",
        "\n",
        "  return conv_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2 as cv\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "img = cv.imread('images/cells.jpg', cv.IMREAD_GRAYSCALE)\n",
        "plt.imshow(img, cmap='gray')\n",
        "plt.show()\n",
        "kernel = np.array([[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]])\n",
        "\n",
        "img_out = convolve_py(img, kernel)\n",
        "print(img_out.min(), img_out.max())\n",
        "plt.imshow(img_out, cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Speeding up convolution with numpy\n",
        "\n",
        "This time, implement the equation above using Numpy functions on matrices to calculate the convolved pixel value as `np.sum(window * kernel)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convolve_np(img, kernel):\n",
        "  '''\n",
        "  Convolves an image with a kernel.\n",
        "  Uses Numpy.\n",
        "\n",
        "  Args:\n",
        "    img: 2D numpy array of the image (grayscale)\n",
        "    kernel: 2D numpy array of the kernel\n",
        "  '''\n",
        "  # Get the dimensions of the image and kernel\n",
        "  img_height, img_width = img.shape\n",
        "  kernel_height, kernel_width = kernel.shape\n",
        "\n",
        "  conv_result = np.zeros(img.shape)\n",
        "  img = np.pad(img, ((1, 1), (1, 1)), 'constant', constant_values=0)\n",
        "\n",
        "  for x in range(img_width):\n",
        "    for y in range(img_height):\n",
        "      # TODO\n",
        "\n",
        "  return conv_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img = cv.imread('images/cells.jpg', cv.IMREAD_GRAYSCALE)\n",
        "img_out = convolve_np(img, kernel)\n",
        "plt.imshow(img_out, cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us check which implementation is faster using the timeit library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import timeit\n",
        "\n",
        "# Time the pure Python version\n",
        "time_py = timeit.timeit('convolve_py(img, kernel)', number=5, globals=globals())\n",
        "print('Pure Python: {:.5f}s'.format(time_py))\n",
        "\n",
        "# Time the dot product version (should be a few seconds faster)\n",
        "time_np = timeit.timeit('convolve_np(img, kernel)', number=5, globals=globals())\n",
        "print('NumPy: {:.5f}s'.format(time_dot))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding kernels\n",
        "\n",
        "Different convolutional kernels can achieve different effects such as:\n",
        "\n",
        " - edge detection\n",
        " - image engancement (e.g. sharpening)\n",
        " - blurring\n",
        " - denoising\n",
        " - feature detection\n",
        "\n",
        "You can see various examples of kernels here:\n",
        "\n",
        " - https://en.wikipedia.org/wiki/Kernel_(image_processing)\n",
        " - https://setosa.io/ev/image-kernels/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Find a good convolutional kernel to perform edge detection on the cell image.\n",
        "\n",
        "kernel_edge = # TODO\n",
        "img_out = cv.filter2D(img, -1, kernel_edge)\n",
        "plt.imshow(img_out, cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Find a kernel to blur the image and show the blurred image.\n",
        "# Then, run the edge detection kernel on the blurred image and show the result.\n",
        "\n",
        "kernel_blur = # TODO\n",
        "# TODO: blur + edge detection, show the result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using large kernels\n",
        "\n",
        "The next code block constructs an image. You job is to recreate the image using a convolution.\n",
        "\n",
        "Hint: https://vincmazet.github.io/bip/filtering/convolution.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img_out = np.zeros((100, 100))\n",
        "circle_origins = [(50, 50), (20, 20), (80, 20)]\n",
        "circle_radius = 10\n",
        "\n",
        "for (x, y) in circle_origins:\n",
        "  img_out = cv.circle(img_out, (x, y), circle_radius, 1, -1)\n",
        "\n",
        "plt.imshow(img_out, cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "kernel = np.zeros((circle_radius * 2 + 1, circle_radius * 2 + 1))\n",
        "kernel = cv.circle(kernel, (circle_radius, circle_radius), circle_radius, 1, -1)\n",
        "plt.imshow(kernel, cmap='gray')\n",
        "\n",
        "img = np.zeros((100, 100))\n",
        "\n",
        "# TODO: Modify `img` such that a convolution img * kernel produces the same result as `img_out`.\n",
        "\n",
        "\n",
        "# OpenCV's built-in convolution function:\n",
        "img = cv.filter2D(img, -1, kernel)\n",
        "plt.imshow(img, cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using Convolutions for Feature Detection\n",
        "\n",
        "Assume we want to detect the eyes on the following image:\n",
        "\n",
        "![](images/woman_darkhair.png)\n",
        "\n",
        "Notice that the eyes consist of a light region and then a dark region (iris). We may be able to detect the eyes using the following kernel:\n",
        "\n",
        "\\begin{bmatrix}\n",
        "1 & 1 & -1 & -1\\\\\n",
        "1 & 1 & -1 & -1\\\\\n",
        "1 & 1 & -1 & -1\\\\\n",
        "1 & 1 & -1 & -1\\\\\n",
        "\\end{bmatrix}\n",
        "\n",
        "This kernel will take the difference between the left two pixels and the right two pixels. If the pixels on the left are high and on the right are low, the resulting value will be high.\n",
        "\n",
        "Implement a convolution with this kernel in the next code block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img = cv.imread('images/woman_darkhair.png', cv.IMREAD_GRAYSCALE)\n",
        "img = cv.resize(img, (0, 0), fx=0.5, fy=0.5)\n",
        "plt.imshow(img, cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "kernel = # TODO: Write the kernel from above as a numpy matrix\n",
        "\n",
        "img_out = cv.filter2D(img, -1, kernel)\n",
        "plt.imshow(img_out, cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's also find the mouth. Guided by the patterns you see in the pixel intensities, try to construct a kernel which will highlight the mouth in the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img = cv.imread('images/woman_darkhair.tif', cv.IMREAD_GRAYSCALE)\n",
        "cv.imwrite('images/woman_darkhair.png', img)\n",
        "\n",
        "kernel = # TODO: Write a kernel to find the mouth\n",
        "\n",
        "img_out = cv.filter2D(img, -1, kernel)\n",
        "plt.imshow(img_out, cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Don't forget to save your tasks to GitHub!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPdsk5aJ45he4xMUxx8bkpd",
      "include_colab_link": true,
      "name": "lab1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
